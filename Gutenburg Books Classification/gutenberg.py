# -*- coding: utf-8 -*-
"""Gutenberg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IdDJFpMFySl64EXYp9icm0OHRkCPGk_9
"""

import random
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer 
from bs4 import BeautifulSoup
import re
import pandas as pd
import numpy as np
import string
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from urllib  import request
from sklearn.model_selection import train_test_split

nltk.download('gutenberg')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words("english"))

"""# 1. Data Preparation

"""

Books = []

URLs = []
URLs.append("http://www.gutenberg.org/files/2554/2554-0.txt")
URLs.append("https://www.gutenberg.org/files/3790/3790.txt")
URLs.append("https://www.gutenberg.org/files/14975/14975.txt")
URLs.append("https://www.gutenberg.org/files/22747/22747-0.txt")
URLs.append("https://www.gutenberg.org/files/65407/65407-0.txt")

for url in URLs:
  Books.append(request.urlopen(url).read().decode('utf8'))

"""1.1) Cleanse
- remove stop words
- remove unnecessary duplicated words
- remove punctuation
1.2) Feature Engineering


"""

author_Names = []
for book in Books:
  author_Names.append(re.findall(r'Author:\s(.(\w+|.)+)', book)[0][0][:-1])

print(author_Names)

part_200 = []
new_words = []
NewWords = []
ps = nltk.stem.porter.PorterStemmer()
lem = nltk.stem.wordnet.WordNetLemmatizer()

# For loop for 200 partions random and each partion contain 100 words 
# preprocess data (text of book) with regex to remove special char
# not in stop_words
tokenizer = RegexpTokenizer("[\w']+")

for book in Books:
  words = re.sub(r'[^\w\s]', '', str(book).lower().strip())
  words = tokenizer.tokenize(words) 

  new_words = []
  for word in words:
      if (word not in stop_words):
          new_words.append(word)

  NewWords = []
  for i in new_words:
      s = ps.stem(i)
      NewWords.append(s)

  for x in range( 200):
      rand =  random.randint(200 , len(NewWords)-3000  )
      partition = NewWords[ rand : rand+100 ]
      
      part_200.append( (' '.join(partition), re.findall(r'Author:\s(.(\w+|.)+)', book)[0][0][:-1]) )

df = pd.DataFrame(part_200 , columns=['Sample', 'Author'])
print(df)

"""#  2. Data Transformation"""

X_train, X_test, y_train, y_test = train_test_split(df['Sample'],df['Author'],test_size=0.2, random_state=0)

"""- BOW
- TF-IDF
- n-gram
- LDA
- Word-embedding
  
     
# - **Doc2Vec**
"""

text = [
  'There was a man',
  'The man had a dog',
  'The dog and the man walked',
  "the sad part is that the arrow and i both dig on flicks like this ",
  "do we really need to see it over and over again",
  "there might've been a pretty decent teen mind-fuck movie in here somewhere",
  "the actors are pretty good for the most part"
]
doc=[
  'There There was a man was a man',
  'The dog There was a man and the man walked The dog and the man walked',
  'The man There was a man had a dog ',
  'The dog and the man walked The dog and the man walked',
]

"""### 2.1) BOW"""

from sklearn.feature_extraction.text import CountVectorizer
BOW_v = CountVectorizer()
X = BOW_v.fit_transform(X_train).todense() 
print(X)
print( BOW_v.vocabulary_ )

'''
vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))
X = vectorizer.fit_transform(documents).toarray()
'''

"""### 2.2) TF_IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
TF_v=TfidfVectorizer()
res=TF_v.fit_transform(X_train).todense()
print(res)

"""### 2.3) N-Gram"""

ngram_v = CountVectorizer(analyzer='word', ngram_range=(2, 2))
res = ngram_v.fit_transform(X_train)
print(ngram_v.get_feature_names())
print(res.toarray())

"""### 2.4) LDA"""

import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
LDA_v=LinearDiscriminantAnalysis()
#LDA_v.fit(text,doc)
##### it needs to be provided by x and y as two arrays of integars
#ValueError: could not convert string to float: 'There was a man'

"""###  2.5) Word Embidding"""

# Word to Vector
from gensim.models import word2vec
tokenized_sentences = [sentence.split() for sentence in df['Sample'].to_numpy()]
word2vec_v = word2vec.Word2Vec(tokenized_sentences, min_count=1)
print(word2vec_v)

"""# 3. Modeling

- SVM 
 - Decision Tree
 - k-Nearest Neighbor
 - Deep-learning optional

Creating different compinations of models and transformations
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm, preprocessing, metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn import metrics
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn import preprocessing
from copy import deepcopy
from sklearn.decomposition import LatentDirichletAllocation

# X = df["Sample"].astype(str).tolist()
'''
X_train = X[:800]
X_test = X[800:]

y = df['Author'].values
y_train = y[:800]
y_test = y[800:]
'''


y_train_labels = y_train.apply(author_Names.index).to_numpy()
y_test_labels = y_test.apply(author_Names.index).to_numpy()

X_train = X_train.to_numpy()
X_test = X_test.to_numpy()
y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

transformations = [
                   [('tran', CountVectorizer())],
                   [('tran1', CountVectorizer()), ('tran2', TfidfTransformer())],
                   [('tran', CountVectorizer(analyzer='word', ngram_range=(2, 2)))],
                  #  [('tran1', CountVectorizer()), ('tran2', LatentDirichletAllocation(max_iter=5))],
                   [('tran1', CountVectorizer()), ('tran2', preprocessing.FunctionTransformer(lambda x: x.todense(), accept_sparse=True)), ('tran3', LinearDiscriminantAnalysis())]
                  ]
classifiers = [
               [('clf', SGDClassifier(max_iter=5))],
               [('clf', KNeighborsClassifier(n_neighbors=5))],
               [('clf', DecisionTreeClassifier(random_state=0))],
               [('clf', svm.SVC())],
               [('clf', MLPClassifier(max_iter=5))]
              #  [('clf', RandomForestClassifier(n_estimators=1000, random_state=0))],
              #  [('clf', svm.SVC(decision_function_shape='ovo'))],
              #  [('clf', svm.LinearSVC())]
              ]
models = []
for transform in transformations:
  for classifier in classifiers:
    text_clf = Pipeline(transform + classifier)
    models.append(deepcopy(text_clf))

"""# 4. Evaluation

10-Fold cross validation with precision of each model
"""

from sklearn.model_selection import KFold

kf = KFold(n_splits=10)
for text_clf in models:
  scores = []
  for train, test in kf.split(X_train):
    model = text_clf.fit(X_train[train], y_train_labels[train])
    scores.append(metrics.precision_score(y_train_labels[test], model.predict(X_train[test]), average='micro'))
    # print(metrics.confusion_matrix(y_train.iloc[test].apply(author_Names.index), model.predict(X_train.iloc[test])))

  print("Precision: %0.2f%%" %(np.array(scores).mean()*100))
  print('-------------------------------------')

"""# **Gridsearch parameter optimization**

Gridsearch for the best parameters of the champion model
"""

from sklearn.model_selection import GridSearchCV

parameters = {
    # SVM
    # 'clf__kernel': ['rbf', 'linear'],
    # 'clf__gamma': [1e-3, 1e-4],
    # 'clf__C': [1, 10, 100, 1000],
    # KNN
    # 'clf__n_neighbors': list(range(1,36))
    # DTree
    # 'clf__criterion': ['gini', 'entropy'],
    # 'clf__max_depth': [2,4,6,8,10,12]
    'tran2__use_idf': (True, False),
    'tran1__ngram_range': [(1, 1), (1, 2), (2,2)],
    'clf__alpha': (1e-2, 1e-3)
}

scorer = metrics.make_scorer(metrics.precision_score, average = 'micro')
gs_clf = GridSearchCV(deepcopy(models[5]), parameters, cv=10, n_jobs=-1, scoring=scorer)

# Fit and evaluate
gs_clf = gs_clf.fit(X_train, y_train_labels)
print("Training precision: " + str(gs_clf.score(X_train, y_train_labels)))
print("Mean cross-validation precision: " + str(gs_clf.best_score_))

print()
print('Best Parameters')
print('----------------')
for param_name in sorted(parameters.keys()):
  print("%s: %r" % (param_name, gs_clf.best_params_[param_name]))

print()
print('Report')
print('-------')
print(metrics.classification_report(y_train_labels, gs_clf.predict(X_train)))

"""# 5. Error Analysis

Testing the model and exploring through visuals.

First, Confusion matrix and FP/FN visualization
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install mlxtend --upgrade

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from mlxtend.evaluate.bias_variance_decomp import bias_variance_decomp

predicted = gs_clf.predict(X_test)
print("Testing precision: " + str(gs_clf.score(X_test, y_test_labels)))

# estimate bias and variance https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/
_, bias, var = bias_variance_decomp(deepcopy(gs_clf), X_train, y_train_labels, X_test, y_test_labels, num_rounds=2)
print('Bias: %.3f' % bias)
print('Variance: %.3f' % var)

cm = metrics.confusion_matrix(y_test_labels, predicted)
metrics.plot_confusion_matrix(gs_clf, X_test, y_test_labels)

# False Negative per Author
group_by_true_label = np.sum(cm, axis=1) - np.diag(cm)
# False Positive per Author
group_by_prediction = np.sum(cm, axis=0) - np.diag(cm)

print()
for i in range(len(author_Names)):
  print(str(group_by_prediction[i]) + ' texts has been wrongly classified as ' + author_Names[i])
for i in range(len(author_Names)):
  print(str(group_by_true_label[i]) + ' of ' + author_Names[i] + ' texts has been wrongly classified')

print()
print(author_Names)
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(author_Names)), group_by_prediction)
plt.title('False Positive per Author')
plt.xticks(range(len(author_Names)), author_Names, rotation=90)

plt.subplot(1, 2, 2)
plt.bar(range(len(author_Names)), group_by_true_label)
plt.title('False Negative per Author')
plt.xticks(range(len(author_Names)), author_Names, rotation=90)
plt.show()

"""Most frequent words

"""

# This code is copied from the FeatureEngineering notebook provided in class with change of variables

# unigrams
nltk.download('punkt')
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))
fig.suptitle("All samples vs Author No. 0 similarities", fontsize=15)
top=10

lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.array(df['Sample'])))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[0], 
                  legend=False).grid(axis='x')
ax[0].set(ylabel=None)

lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.array(df.loc[df['Author'] == author_Names[0]]['Sample'])))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[1], 
                  legend=False).grid(axis='x')
ax[1].set(ylabel=None)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))
fig.suptitle("Author 1 vs 2", fontsize=15)

lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.array(df.loc[df['Author'] == author_Names[1]]['Sample'])))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[0], 
                  legend=False).grid(axis='x')
ax[0].set(ylabel=None)
lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.array(df.loc[df['Author'] == author_Names[2]]['Sample'])))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[1], 
                  legend=False).grid(axis='x')
ax[1].set(ylabel=None)


fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))
fig.suptitle("Author 3 vs 4", fontsize=15)

lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.array(df.loc[df['Author'] == author_Names[3]]['Sample'])))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[0], 
                  legend=False).grid(axis='x')
ax[0].set(ylabel=None)
lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.array(df.loc[df['Author'] == author_Names[4]]['Sample'])))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[1], 
                  legend=False).grid(axis='x')
ax[1].set(ylabel=None)

plt.show()

"""**Before treating the erros**

Authors 0 and 2 has no FN, the reason maybe is because they have words that are not common and unique to them in their lists of the most frequent word. For example, author 0 has 'raskolnikov' and author 2 has 'afroamerican' and 'gutenbergtm'.

That could mean that the model is biased towards authors that use unique words, and espicially Author 2.

**After treating the errors**

The models show 100% precision and has no wrong predictions.

Let's see the samples that threw off the model for further analyzation.
"""

for input, prediction, label in zip(X_test, predicted, y_test_labels):
  if prediction != label:
    print(input[:70] + '... has been classified as ', prediction, 'and should be ', label) 

    fig, ax = plt.subplots()
    fig.suptitle("Classified as " + str(prediction) + " and should be " + str(label), fontsize=15)

    lst_tokens = nltk.tokenize.word_tokenize(input)
    dic_words_freq = nltk.FreqDist(lst_tokens)
    dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                          columns=["Word","Freq"])
    dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                      kind="barh", title="Unigrams", ax=ax, 
                      legend=False).grid(axis='x')
    ax.set(ylabel=None)

"""**Before treating the erros**

We can see that False Positives of Author 2 have some frequent words that are the same of the top frequent words for Author 2. Also they use words like 'gutenberg' which is a little unique for Author 2.

**After treating the erros**

The model shows 100% precision and has no wrong predictions.

# 6. Verification

>  Reduce the number of words per document if the accuracy is too high and then repeat the above steps.
"""

X_train_less = np.array([' '.join(tokenizer.tokenize(sample)[:-80]) for sample in X_train])
X_test_less = np.array([' '.join(tokenizer.tokenize(sample)[:-80]) for sample in X_test])

parameters_less = {
    # SVM
    # 'clf__kernel': ['rbf', 'linear'],
    # 'clf__gamma': [1e-3, 1e-4],
    # 'clf__C': [1, 10, 100, 1000],
    # KNN
    # 'clf__n_neighbors': list(range(1,36))
    # DTree
    # 'clf__criterion': ['gini', 'entropy'],
    # 'clf__max_depth': [2,4,6,8,10,12]
    # 'tran2__use_idf': (True, False),
    'tran1__ngram_range': [(1, 1), (1, 2), (2,2)],
    # 'clf__alpha': (1e-2, 1e-3)
}

scorer_less = metrics.make_scorer(metrics.precision_score, average = 'micro')
gs_clf_less = GridSearchCV(deepcopy(models[19]), parameters_less, cv=10, n_jobs=-1, scoring=scorer_less)

# Fit and evaluate
gs_clf_less = gs_clf_less.fit(X_train_less, y_train_labels)
print("Training precision: " + str(gs_clf_less.score(X_train_less, y_train_labels)))
print("Mean cross-validation precision: " + str(gs_clf_less.best_score_))

print()
print('Best Parameters')
print('----------------')
for param_name in sorted(parameters_less.keys()):
  print("%s: %r" % (param_name, gs_clf_less.best_params_[param_name]))

print()
print('Report')
print('-------')
print(metrics.classification_report(y_train_labels, gs_clf_less.predict(X_train)))

predicted = gs_clf_less.predict(X_test_less)
print("Testing precision: " + str(gs_clf_less.score(X_test_less, y_test_labels)))

# estimate bias and variance https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/
_, bias, var = bias_variance_decomp(deepcopy(gs_clf_less), X_train_less, y_train_labels, X_test_less, y_test_labels, num_rounds=2)
print('Bias: %.3f' % bias)
print('Variance: %.3f' % var)

cm = metrics.confusion_matrix(y_test_labels, predicted)
metrics.plot_confusion_matrix(gs_clf_less, X_test_less, y_test_labels)

# False Negative per Author
group_by_true_label = np.sum(cm, axis=1) - np.diag(cm)
# False Positive per Author
group_by_prediction = np.sum(cm, axis=0) - np.diag(cm)

print()
for i in range(len(author_Names)):
  print(str(group_by_prediction[i]) + ' texts has been wrongly classified as ' + author_Names[i])
for i in range(len(author_Names)):
  print(str(group_by_true_label[i]) + ' of ' + author_Names[i] + ' texts has been wrongly classified')

print()
print(author_Names)
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.bar(range(len(author_Names)), group_by_prediction)
plt.title('False Positive per Author')
plt.xticks(range(len(author_Names)), author_Names, rotation=90)

plt.subplot(1, 2, 2)
plt.bar(range(len(author_Names)), group_by_true_label)
plt.title('False Negative per Author')
plt.xticks(range(len(author_Names)), author_Names, rotation=90)
plt.show()

"""Having less words caused the model to over fit. It might have high accuracy in training but when it comes to testing, the percentage is low.

Most frequent words
"""

# This code is copied from the FeatureEngineering notebook provided in class with change of variables

# unigrams
nltk.download('punkt')
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))
fig.suptitle("All samples vs Author No. 0 similarities", fontsize=15)
top=10

lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.append(X_train_less, X_test_less)))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[0], 
                  legend=False).grid(axis='x')
ax[0].set(ylabel=None)

lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.append(X_train_less, X_test_less)[df['Author'] == author_Names[0]]))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[1], 
                  legend=False).grid(axis='x')
ax[1].set(ylabel=None)

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))
fig.suptitle("Author 1 vs 2", fontsize=15)

lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.append(X_train_less, X_test_less)[df['Author'] == author_Names[1]]))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[0], 
                  legend=False).grid(axis='x')
ax[0].set(ylabel=None)
lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.append(X_train_less, X_test_less)[df['Author'] == author_Names[2]]))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[1], 
                  legend=False).grid(axis='x')
ax[1].set(ylabel=None)


fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))
fig.suptitle("Author 3 vs 4", fontsize=15)

lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.append(X_train_less, X_test_less)[df['Author'] == author_Names[3]]))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[0], 
                  legend=False).grid(axis='x')
ax[0].set(ylabel=None)
lst_tokens = nltk.tokenize.word_tokenize(' '.join(np.append(X_train_less, X_test_less)[df['Author'] == author_Names[4]]))
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[1], 
                  legend=False).grid(axis='x')
ax[1].set(ylabel=None)

plt.show()

"""**Examples that threw off the machine**"""

for input, prediction, label in zip(X_test_less, predicted, y_test_labels):
  if prediction != label:
    print(input[:70] + '... has been classified as ', prediction, 'and should be ', label) 

    fig, ax = plt.subplots()
    fig.suptitle("Classified as " + str(prediction) + " and should be " + str(label), fontsize=15)

    lst_tokens = nltk.tokenize.word_tokenize(input)
    dic_words_freq = nltk.FreqDist(lst_tokens)
    dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                          columns=["Word","Freq"])
    dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                      kind="barh", title="Unigrams", ax=ax, 
                      legend=False).grid(axis='x')
    ax.set(ylabel=None)

"""Now the most frequent words for each other have more common words and we belive that threw off the machine.

The bias and variance are higher specially the bias. That is because the model is underfitted.
"""